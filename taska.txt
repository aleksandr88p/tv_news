что мне надо сделать
из некоторых источников спарсить статьи, хранить их в бд, после этого обработать перед использованием
chatGPT, после этого как то предоставлять заказзчику сводку по новостям
https://www.hollywoodreporter.com
The Hollywood Reporter (Film News, and TV News)

https://variety.com
Variety (Film News, and TV News)

https://www.indiewire.com
IndieWire (Film and TV)

https://deadline.com
Deadline (Film and TV)


Скрапинг данных:
для начала напишу скрипты для извлечения статей. Попробую для начала использовать просто requests или aiiohttp(если
нужна будет ассинхронность) попробую сначала все это делать без использования прокси серверов, надеюсь не будет блокировок
Нужно сделать первый запрос, что бы посмотерть и сохранить ссылки на статьи. Так что я думаю на этом этапе записывать
в первую таблицу, заголовок и ссылку на статью


Хранение данных:
буду хранить статьи в БД(mysql или postgresql)
планирую сделать одну БД, и одну таблицу для каждого источника с колонками (дата новости, источник, статья и т.д.)

Обработка данных:
возможно придется обработать статьи перед использованием в chatGPT,это может включать в себя очистку текста,
такую как удаление специальных символов, ссылок или другого несущественного содержания.
библиотеки NLP, такие как NLTK или Spacy.

Использование ChatGPT для создания сводок: Затем использовать ChatGPT для создания сводок для каждой статьи.
Это можно сделать, используя API предложенное OpenAI,
и передавая текст статьи в качестве ввода для получения краткого содержания.

Компиляция и представление данных: Последним шагом будет компиляция сводок в единый дайджест и
представление этого дайджеста в удобном для чтения формате. например,
создать веб-страницу с помощью библиотеки Flask, чтобы представить эти сводки.

Автоматизация: Весь этот процесс можно автоматизировать,
используя cronjobs (если вы используете Unix-подобную операционную систему)
или с помощью Task Scheduler (для Windows). Вы можете настроить эту работу, чтобы выполняться каждый день в определенное время.


я думаю весь этот проект написать в fast api
у меня там надо парсить около 8 страниц.
1) Как мне в структуре fastApi это сделать. Типа в главный файл импортировать мои парсеры,
2) а как авмтоматизировать мою апишку. типа что бы каждый час парсил




может тогда мне не нужна таблица, в которой только ссылки будут? сразу хранить все в одной таблице, и там уже проверять

