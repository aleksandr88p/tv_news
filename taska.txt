что мне надо сделать
из некоторых источников спарсить статьи, хранить их в бд, после этого обработать перед использованием
chatGPT, после этого как то предоставлять заказзчику сводку по новостям
https://www.hollywoodreporter.com
The Hollywood Reporter (Film News, and TV News)

https://variety.com
Variety (Film News, and TV News)

https://www.indiewire.com
IndieWire (Film and TV)

https://deadline.com
Deadline (Film and TV)


Скрапинг данных:
для начала напишу скрипты для извлечения статей. Попробую для начала использовать просто requests или aiiohttp(если
нужна будет ассинхронность) попробую сначала все это делать без использования прокси серверов, надеюсь не будет блокировок
Нужно сделать первый запрос, что бы посмотерть и сохранить ссылки на статьи. Так что я думаю на этом этапе записывать
в первую таблицу, заголовок и ссылку на статью


Хранение данных:
буду хранить статьи в БД(mysql или postgresql)
планирую сделать одну БД, и одну таблицу для каждого источника с колонками (дата новости, источник, статья и т.д.)

Обработка данных:
возможно придется обработать статьи перед использованием в chatGPT,это может включать в себя очистку текста,
такую как удаление специальных символов, ссылок или другого несущественного содержания.
библиотеки NLP, такие как NLTK или Spacy.

Использование ChatGPT для создания сводок: Затем использовать ChatGPT для создания сводок для каждой статьи.
Это можно сделать, используя API предложенное OpenAI,
и передавая текст статьи в качестве ввода для получения краткого содержания.

Компиляция и представление данных: Последним шагом будет компиляция сводок в единый дайджест и
представление этого дайджеста в удобном для чтения формате. например,
создать веб-страницу с помощью библиотеки Flask, чтобы представить эти сводки.

Автоматизация: Весь этот процесс можно автоматизировать,
используя cronjobs (если вы используете Unix-подобную операционную систему)
или с помощью Task Scheduler (для Windows). Вы можете настроить эту работу, чтобы выполняться каждый день в определенное время.


я думаю весь этот проект написать в fast api
у меня там надо парсить около 8 страниц.
1) Как мне в структуре fastApi это сделать. Типа в главный файл импортировать мои парсеры,
2) а как авмтоматизировать мою апишку. типа что бы каждый час парсил


Написание кода, который идентифицирует перекрывающиеся статьи на основе ключевых слов или тем из каждого источника.
Подача этих статей на вход GPT-3 для генерации коротких резюме.
Настройка параметров GPT-3, чтобы добиться желаемого тона и стиля.



как происходит фильтрация
Сбор данных: Сначала мы извлекаем все статьи из каждой из восьми баз данных. Мы сохраняем текст каждой статьи, её URL (для уникальной идентификации), название и источник.

Предобработка текста: Затем мы проходим через каждую статью и удаляем все так называемые "стоп-слова" (часто встречающиеся слова, которые не несут много смысла, такие как "и", "в", "на" и т.д.). Это помогает сфокусироваться на более важных словах в каждой статье.

Векторизация статей: Далее мы преобразуем каждую статью в вектор признаков. Векторизация - это процесс преобразования текста в числовой формат, который может быть анализирован алгоритмами машинного обучения. Мы используем метод, называемый TF-IDF (Term Frequency-Inverse Document Frequency), который учитывает не только количество вхождений каждого слова в статье, но и то, насколько часто оно встречается в других статьях.

Сравнение статей: Затем мы сравниваем каждую статью со всеми остальными, используя косинусное сходство. Это мера, которая позволяет нам определить, насколько похожи два вектора (в нашем случае, две статьи). Мы устанавливаем порог сходства - если сходство двух статей выше этого порога, мы считаем их похожими.

Группировка похожих статей: Мы создаем группы похожих статей. Если статья A похожа на статью B, а статья B похожа на статью C, мы группируем их все вместе, даже если статья A и C не превышают порога сходства напрямую.

Фильтрация групп: Наконец, мы отфильтровываем группы, чтобы выводить только те, которые содержат статьи из разных источников. Если количество уникальных источников в группе меньше половины от общего количества источников, мы пропускаем эту группу.



